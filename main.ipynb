{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling TRAIN_TT by 100.0\n",
      "scaling TRAIN_CO by 100.0\n",
      "scaling SM_TT by 100.0\n",
      "scaling SM_CO by 100.0\n",
      "scaling CAR_TT by 100.0\n",
      "scaling CAR_CO by 100.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dill as pickle\n",
    "import aesara\n",
    "import aesara.tensor as aet\n",
    "from biogeme.database import Database\n",
    "import pandas as pd\n",
    "from biogeme_aesara import DatabaseShared, BetaShared\n",
    "\n",
    "swissmetro = pd.read_csv(\"examples/swissmetro/swissmetro.dat\", sep=\"\\t\")\n",
    "db = DatabaseShared(\"swissmetro\", swissmetro, choiceVar=\"CHOICE\")\n",
    "globals().update(db.variables)\n",
    "\n",
    "# Removing some observations\n",
    "exclude = ((PURPOSE != 1) * (PURPOSE != 3) + (CHOICE == 0)) > 0\n",
    "db.remove(exclude)\n",
    "db.data[\"CHOICE\"] -= 1 # set the first choice to 0\n",
    "db.autoscale(variables=['TRAIN_CO', 'TRAIN_TT', 'CAR_CO', 'CAR_TT', \n",
    "    'SM_CO', 'SM_TT'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnl_model:\n",
    "    def __init__(self):\n",
    "        b_cost = BetaShared(\"b_cost\", 0., None, None, 0)\n",
    "        b_time = BetaShared(\"b_time\", 0., None, None, 0)\n",
    "        asc_train = BetaShared(\"asc_train\", 0., None, None, 0)\n",
    "        asc_car = BetaShared(\"asc_car\", 0., None, None, 0)\n",
    "        asc_sm = BetaShared(\"asc_sm\", 0., None, None, 0)\n",
    "        \n",
    "        self.y = CHOICE.y\n",
    "        self.params = [b_cost(), b_time(), asc_train(), asc_car()]\n",
    "        self.full_params = [b_cost(), b_time(), asc_train(), asc_car(), asc_sm()]\n",
    "\n",
    "        # Definition of the utility functions\n",
    "        U_1 = b_cost * TRAIN_CO.x + b_time * TRAIN_TT.x + asc_train\n",
    "        U_2 = b_cost * SM_CO.x + b_time * SM_TT.x + asc_sm\n",
    "        U_3 = b_cost * CAR_CO.x + b_time * CAR_TT.x + asc_car\n",
    "\n",
    "        utility_vector = aet.concatenate([U_1, U_2, U_3], axis=1)\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        self.p_y_given_x = aet.nnet.softmax(utility_vector, axis=1)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.pred = aet.argmax(self.p_y_given_x, axis=1)\n",
    "\n",
    "        self.L1 = aet.sum([b_cost(), b_time()])\n",
    "        \n",
    "\n",
    "def neg_loglikelihood(prob, y):\n",
    "    nll = -aet.mean(aet.log(prob)[aet.arange(y.shape[0]), y])\n",
    "    return nll\n",
    "\n",
    "\n",
    "def full_loglikelihood(prob, y):\n",
    "    ll = aet.sum(aet.log(prob)[aet.arange(y.shape[0]), y])\n",
    "    return ll\n",
    "\n",
    "\n",
    "def errors(pred, y):\n",
    "    if y.ndim != pred.ndim:\n",
    "        raise TypeError(\n",
    "            'y should have the same shape as pred',\n",
    "            ('y', y.type, 'pred', pred.type)\n",
    "        )\n",
    "    if y.dtype.startswith('int'):\n",
    "        return aet.mean(aet.neq(pred, y))\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, b1=0.9, b2=0.999, epsilon=1e-8):\n",
    "        zero = np.array(0.).astype(aesara.config.floatX)\n",
    "        self.m = [aesara.shared(p.get_value() * zero) for p in params]\n",
    "        self.v = [aesara.shared(p.get_value() * zero) for p in params]\n",
    "        self._t_prev = aesara.shared(zero)\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, cost, params, lr=0.0001):\n",
    "        one = np.array(1.).astype(aesara.config.floatX)\n",
    "        grads = aet.grad(cost, params)\n",
    "        t_prev = self._t_prev\n",
    "        \n",
    "        updates = []\n",
    "        m_prev = self.m\n",
    "        v_prev = self.v\n",
    "        t = t_prev + 1.\n",
    "        \n",
    "        a_t = aet.sqrt(one-self.b2**t)/(one-self.b1**t)\n",
    "        for m, v, param, grad in zip(m_prev, v_prev, params, grads):\n",
    "            m_t = self.b1 * m + (one-self.b1) * grad\n",
    "            v_t = self.b2 * v + (one-self.b2) * grad**2\n",
    "            g_t = a_t * m_t / (aet.sqrt(v_t) + self.epsilon)\n",
    "            p_t = param - lr * g_t\n",
    "            \n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((param, p_t))\n",
    "        \n",
    "        updates.append((t_prev, t))\n",
    "        return updates\n",
    "\n",
    "class RMSProp:\n",
    "    def __init__(self, params, rho=0.9, epsilon=1e-8, decay=0.):\n",
    "        zero = np.array(0.).astype(aesara.config.floatX)\n",
    "        self._a = [aesara.shared(p.get_value() * zero) for p in params]\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "\n",
    "    def update(self, cost, params, lr=0.001):\n",
    "        grads = aet.grad(cost, params)\n",
    "\n",
    "        updates = []\n",
    "        accumulators = self._a\n",
    "        for a, param, grad in zip(accumulators, params, grads):\n",
    "            a_t = self.rho * a + (1. - self.rho) * grad**2\n",
    "            g_t = grad / (aet.sqrt(a_t) + self.epsilon)\n",
    "            p_t = param - lr * g_t\n",
    "\n",
    "            updates.append((a, a_t))\n",
    "            updates.append((param, p_t))\n",
    "        \n",
    "        return updates\n",
    "\n",
    "class MomentumSGD:\n",
    "    def __init__(self, params, momentum=0.9, nesterov=False):\n",
    "        zero = np.array(0.).astype(aesara.config.floatX)\n",
    "        self._moments = [aesara.shared(p.get_value() * zero) for p in params]\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "\n",
    "    def update(self, cost, params, lr=0.001):\n",
    "        grads = aet.grad(cost, params)\n",
    "\n",
    "        updates = []\n",
    "        moments = self._moments\n",
    "        for m, param, grad in zip(moments, params, grads):\n",
    "            velocity = self.momentum * m - lr * grad\n",
    "\n",
    "            if self.nesterov:\n",
    "                p_t = param + self.momentum * velocity - lr * grad\n",
    "            else:\n",
    "                p_t = param + velocity\n",
    "            \n",
    "            updates.append((m, velocity))\n",
    "            updates.append((param, p_t))\n",
    "        return updates\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params=None):\n",
    "        pass\n",
    "\n",
    "    def update(self, cost, params, lr=0.00001):\n",
    "        grads = aet.grad(cost, params)\n",
    "\n",
    "        updates = []\n",
    "        for param, grad in zip(params, grads):\n",
    "            p_t = param - lr * grad\n",
    "            updates.append((param, p_t))\n",
    "        return updates\n",
    "\n",
    "learning_rate = aet.scalar(\"learning_rate\")\n",
    "model = mnl_model()\n",
    "cost = neg_loglikelihood(model.p_y_given_x, model.y) + 0.01*model.L1\n",
    "\n",
    "opt = Adam(model.params)\n",
    "updates = opt.update(cost=cost, params=model.params, lr=learning_rate)\n",
    "\n",
    "loglikelihood_estimation = aesara.function(\n",
    "    inputs=db.inputs() + [learning_rate],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    on_unused_input=\"ignore\",\n",
    ")\n",
    "\n",
    "loglikelihood = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=full_loglikelihood(model.p_y_given_x, model.y),\n",
    "    on_unused_input=\"ignore\",\n",
    ")\n",
    "\n",
    "output_probabilities = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=model.p_y_given_x,\n",
    "    on_unused_input=\"ignore\",\n",
    ")\n",
    "\n",
    "output_estimated_betas = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=model.full_params,\n",
    "    on_unused_input=\"ignore\",\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches per epoch: 211\n",
      "batch size 32\n",
      "validation frequency 211\n",
      "{b_cost: 0.176436, b_time: -0.72008, asc_train: -0.55205, asc_car: -0.275178, asc_sm: 0.0}\n",
      "epoch 1, minibatch 211/211, LL: -6573.204438, progress  2.62%\n",
      "{b_cost: 0.225013, b_time: -1.358846, asc_train: -0.934934, asc_car: -0.412618, asc_sm: 0.0}\n",
      "epoch 2, minibatch 211/211, LL: -6294.171312, progress  5.26%\n",
      "{b_cost: 0.234944, b_time: -1.867785, asc_train: -1.03369, asc_car: -0.529747, asc_sm: 0.0}\n",
      "epoch 3, minibatch 211/211, LL: -6231.754555, progress  7.90%\n",
      "{b_cost: 0.210407, b_time: -2.284235, asc_train: -1.133056, asc_car: -0.570314, asc_sm: 0.0}\n",
      "epoch 4, minibatch 211/211, LL: -6206.182972, progress 10.54%\n",
      "{b_cost: 0.203439, b_time: -2.381459, asc_train: -1.147321, asc_car: -0.591919, asc_sm: 0.0}\n",
      "epoch 5, minibatch 211/211, LL: -6202.971119, progress 13.18%\n",
      "{b_cost: 0.214044, b_time: -2.466934, asc_train: -1.14379, asc_car: -0.606585, asc_sm: 0.0}\n",
      "epoch 6, minibatch 211/211, LL: -6201.429587, progress 15.81%\n",
      "{b_cost: 0.192159, b_time: -2.568086, asc_train: -1.164133, asc_car: -0.597894, asc_sm: 0.0}\n",
      "epoch 7, minibatch 211/211, LL: -6200.791960, progress 18.45%\n",
      "{b_cost: 0.199051, b_time: -2.636755, asc_train: -1.159025, asc_car: -0.586898, asc_sm: 0.0}\n",
      "epoch 8, minibatch 211/211, LL: -6200.574109, progress 21.09%\n",
      "{b_cost: 0.195081, b_time: -2.735403, asc_train: -1.183404, asc_car: -0.575152, asc_sm: 0.0}\n",
      "epoch 9, minibatch 211/211, LL: -6199.873206, progress 23.72%\n",
      "{b_cost: 0.225052, b_time: -2.826484, asc_train: -1.17632, asc_car: -0.615143, asc_sm: 0.0}\n",
      "epoch 10, minibatch 211/211, LL: -6196.674229, progress 26.36%\n",
      "{b_cost: 0.211103, b_time: -2.916885, asc_train: -1.187257, asc_car: -0.617565, asc_sm: 0.0}\n",
      "epoch 11, minibatch 211/211, LL: -6196.660626, progress 29.00%\n",
      "Optimization complete with best validation score of 100.000%, with negative loglikelihood of -6196.660626, at epoch #11\n",
      "{b_cost: 0.2111, b_time: -2.9169, asc_train: -1.1873, asc_car: -0.6176, asc_sm: 0.0}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_batches = db.data[\"CHOICE\"].shape[0] // batch_size\n",
    "print(\"batches per epoch:\", n_batches)\n",
    "print(\"batch size\", batch_size)\n",
    "\n",
    "nll_best = np.inf\n",
    "error_best = 1.\n",
    "ll_best = -np.inf\n",
    "patience = 8000\n",
    "patience_increase = 2\n",
    "validation_frequency = min(n_batches, patience/2)\n",
    "print(\"validation frequency\", validation_frequency)\n",
    "max_epoch = 50000\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "null_ll = loglikelihood(*(db.input_data()))\n",
    "while (epoch < max_epoch) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in range(n_batches):\n",
    "        r = np.random.randint(0, n_batches)\n",
    "        s = np.random.randint(0, batch_size)\n",
    "        iter = (epoch - 1) * n_batches + minibatch_index\n",
    "        if (iter/patience) < 0.6:\n",
    "            if (iter/patience) < 0.1:\n",
    "                lr = 0.005\n",
    "            else:\n",
    "                lr = 0.001\n",
    "        else:\n",
    "            lr = 0.0001\n",
    "        nll = loglikelihood_estimation(\n",
    "            *(db.input_data(r, batch_size, s)) + [lr])\n",
    "        \n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            ll = loglikelihood(*(db.input_data()))\n",
    "            if ll > ll_best:\n",
    "                if ll > (ll_best * 1.005):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                    estimated_betas = {}\n",
    "                    for n, p in enumerate(model.full_params):\n",
    "                        bb = output_estimated_betas(*(db.input_data()))\n",
    "                        estimated_betas[p] = np.round(bb[n].tolist(), 6)\n",
    "                    print(estimated_betas)\n",
    "                    print((\n",
    "                        \"epoch {0}, minibatch {1}/{2}, LL: {3:6.6f}, \"\n",
    "                        \"progress {4:5.2f}%\").format(\n",
    "                        epoch, minibatch_index + 1, n_batches, ll, iter/patience * 100\n",
    "                    ))\n",
    "                ll_best = ll\n",
    "                epoch_best = epoch\n",
    "                betas_best = output_estimated_betas(*(db.input_data()))\n",
    "                with open('best_model.pkl', 'wb') as f:\n",
    "                    pickle.dump(model, f)\n",
    "                \n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "with open('best_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "output_errors = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=errors(model.pred, model.y),\n",
    "    on_unused_input=\"ignore\",\n",
    ")\n",
    "error_best = output_errors(*(db.input_data()))\n",
    "\n",
    "print(\n",
    "    (\n",
    "        \"Optimization complete with best validation score of {0:6.3f}%, \"\n",
    "        \"with negative loglikelihood of {1:9.6f}, at epoch #{2}\"\n",
    "    ).format(error_best * 100., ll_best, epoch_best)\n",
    ")\n",
    "\n",
    "output_estimated_betas = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=model.full_params,\n",
    "    on_unused_input=\"ignore\",\n",
    ") \n",
    "estimated_betas = {}\n",
    "for n, p in enumerate(model.full_params):\n",
    "    estimated_betas[p] = np.round(betas_best[n].tolist(), 4)\n",
    "print(estimated_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessians\n",
      " [[-2106.05301544   -15.04348334    21.7719284    523.64346   ]\n",
      " [  -15.04348334   -12.77181711   -53.91756945   -22.31861411]\n",
      " [   21.7719284    -53.91756945  -777.81529269   228.57886113]\n",
      " [  523.64346      -22.31861411   228.57886113 -1262.73149407]]\n",
      "BHHH\n",
      " [[5798.21651409 1007.34754969  653.12165767 3055.02786274]\n",
      " [1007.34754969  175.01055426  113.46946081  530.76231705]\n",
      " [ 653.12165767  113.46946081   73.56881184  344.12389691]\n",
      " [3055.02786274  530.76231705  344.12389691 1609.66656203]]\n",
      "[b_cost, b_time, asc_train, asc_car]\n",
      "[0.20147672252093407, -3.8721094252865784, -1.1847555494628468, -0.636721105403731]\n",
      "standard errors:\n",
      " [0.024062 0.372237 0.047879 0.034082]\n",
      "t-test\n",
      " [  8.373072 -10.402274 -24.744969 -18.681881]\n",
      "rob. standard errors:\n",
      " [0.02722  1.32577  0.078521 0.005414]\n",
      "rob. t-test\n",
      " [   7.401702   -2.92065   -15.088396 -117.599695]\n",
      "correlation matrix\n",
      " [[ 1.         -0.27683221  0.24892839  0.40951735]\n",
      " [-0.27683221  1.         -0.63388097 -0.43528266]\n",
      " [ 0.24892839 -0.63388097  1.          0.44953986]\n",
      " [ 0.40951735 -0.43528266  0.44953986  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def hessian(prob, y, params):\n",
    "    grads = aet.grad(full_loglikelihood(prob, y), params)\n",
    "    _h = aet.as_tensor_variable(np.zeros((len(grads), len(grads))))\n",
    "    for i in range(len(grads)):\n",
    "        _h = aet.set_subtensor(x=_h[i,:], y=aet.grad(grads[i], model.params, \n",
    "\t\t    consider_constant=None, disconnected_inputs=\"ignore\"))\n",
    "    return _h\n",
    "\n",
    "def bh(prob, y, params):\n",
    "    grads = aet.grad(full_loglikelihood(prob, y), params)\n",
    "    _bh = aet.outer(aet.as_tensor_variable(grads), aet.as_tensor_variable(grads).T)\n",
    "    return _bh\n",
    "\n",
    "def variance_covariance(h):\n",
    "    return -linalg.pinv(np.nan_to_num(h))\n",
    "\n",
    "def rob_variance_covariance(h, bhhh):\n",
    "    varcovar = variance_covariance(h)\n",
    "    return varcovar.dot(bhhh.dot(varcovar))\n",
    "\n",
    "def t_test(stderr, params):\n",
    "    return [p.eval()/s for p, s in zip(params, stderr)]\n",
    "\n",
    "def std_error(h, params):\n",
    "    varcovar = variance_covariance(h)\n",
    "    stderr = []\n",
    "    for i in range(len(params)):\n",
    "        if varcovar[i, i] < 0:\n",
    "            stderr.append(np.finfo(float).max)\n",
    "        else:\n",
    "            stderr.append(np.sqrt(varcovar[i, i]))\n",
    "    return stderr\n",
    "\n",
    "def correlation_matrix(h):\n",
    "    varcovar = variance_covariance(h)\n",
    "    d = np.diag(varcovar)\n",
    "    if (d > 0).all():\n",
    "        diag = np.diag(np.sqrt(d))\n",
    "        diagInv = linalg.inv(diag)\n",
    "        correlation = diagInv.dot(varcovar.dot(diagInv))\n",
    "    else:\n",
    "        correlation = np.full_like(varcovar, np.finfo(float).max)\n",
    "    return correlation\n",
    "\n",
    "def rob_std_error(h, bhhh, params):\n",
    "    varcovar = variance_covariance(h)\n",
    "    robust_varcovar = varcovar.dot(bhhh.dot(varcovar))\n",
    "    robstderr = []\n",
    "    for i in range(len(params)):\n",
    "        if robust_varcovar[i, i] < 0:\n",
    "            robstderr.append(np.finfo(float).max)\n",
    "        else:\n",
    "            robstderr.append(np.sqrt(robust_varcovar[i, i]))\n",
    "    return robstderr\n",
    "\n",
    "def rob_correlation_matrix(h, bhhh):\n",
    "    robust_varcovar = rob_variance_covariance(h, bhhh)\n",
    "    rd = np.diag(robust_varcovar)\n",
    "    if (rd > 0).all():\n",
    "        diag = np.diag(np.sqrt(rd))\n",
    "        diagInv = linalg.inv(diag)\n",
    "        robust_correlation = diagInv.dot(robust_varcovar.dot(diagInv))\n",
    "    else:\n",
    "        robust_correlation = np.full_like(robust_varcovar, np.finfo(float).max)\n",
    "    return robust_correlation\n",
    "\n",
    "H = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=hessian(model.p_y_given_x, model.y, model.params),\n",
    "    on_unused_input=\"ignore\",\n",
    ")\n",
    "\n",
    "BHHH = aesara.function(\n",
    "    inputs=db.inputs(),\n",
    "    outputs=bh(model.p_y_given_x, model.y, model.params),\n",
    "    on_unused_input=\"ignore\",\n",
    ")\n",
    "\n",
    "hessians = H(*(db.input_data()))\n",
    "bhhh = BHHH(*(db.input_data()))\n",
    "print(\"Hessians\\n\", hessians)\n",
    "print(\"BHHH\\n\", bhhh)\n",
    "\n",
    "# Print parameters\n",
    "print([param for param in model.params])\n",
    "print([param.eval()*1. for param in model.params])\n",
    "# Standard errors\n",
    "stderr = std_error(hessians, model.params)\n",
    "print(\"standard errors:\\n\", np.round(stderr, 6))\n",
    "\n",
    "ttest = t_test(stderr, model.params)\n",
    "print(\"t-test\\n\", np.round(ttest, 6))\n",
    "\n",
    "# Robust standard errors\n",
    "robstderr = rob_std_error(hessians, bhhh, model.params)\n",
    "print(\"rob. standard errors:\\n\", np.round(robstderr, 6))\n",
    "\n",
    "ttest = t_test(robstderr, model.params)\n",
    "print(\"rob. t-test\\n\", np.round(ttest, 6))\n",
    "\n",
    "print(\"correlation matrix\\n\", correlation_matrix(hessians))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca67eba6a02de5a4596a46c03ec5b361b0293be7b73fa28b451703ced66c5cb4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
